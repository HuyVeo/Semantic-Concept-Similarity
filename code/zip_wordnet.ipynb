{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Gộp word với gloss",
   "id": "80d7442200cf230"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-14T14:00:48.857899Z",
     "start_time": "2025-10-14T14:00:48.717812Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã gộp xong! File kết quả: D:\\Semantic-Concept-Similarity\\data\\WordNet\\WordNet.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\S'\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_9716\\840028965.py:1: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  file_words = \"D:\\Semantic-Concept-Similarity\\data\\WordNet\\Words.txt\"\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_9716\\840028965.py:2: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  file_gloss = \"D:\\Semantic-Concept-Similarity\\data\\WordNet\\gloss_tach.txt\"\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_9716\\840028965.py:3: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  output_file = \"D:\\Semantic-Concept-Similarity\\data\\WordNet\\WordNet.txt\"\n"
     ]
    }
   ],
   "execution_count": 2,
   "source": [
    "file_words = \"/data/WordNet/WordNet_gốc/Words.txt\"\n",
    "file_gloss = \"/data/WordNet/WordNet_gốc/gloss_tach.txt\"\n",
    "output_file = \"D:\\Semantic-Concept-Similarity\\data\\WordNet\\WordNet.txt\"\n",
    "\n",
    "with open(file_words, \"r\", encoding=\"utf-8\") as f1, open(file_gloss, \"r\", encoding=\"utf-8\") as f2:\n",
    "    words = f1.readlines()\n",
    "    glosses = f2.readlines()\n",
    "\n",
    "merged_lines = []\n",
    "for w, g in zip(words, glosses):\n",
    "    merged_lines.append(f\"{w.strip()}\\t{g.strip()}\\n\")\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as out:\n",
    "    out.writelines(merged_lines)\n",
    "\n",
    "print(output_file)\n"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Truy vấn list word Vsim-400 vào kho tri thứ WordNet",
   "id": "18b22665e9d57209"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tiền xử lý dử liệu đầu vào",
   "id": "8cb91270f6b053f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T00:45:51.526716Z",
     "start_time": "2025-10-17T00:45:50.986424Z"
    }
   },
   "cell_type": "code",
   "source": "import pandas as pd",
   "id": "dd867f3a63475319",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T00:46:55.385938Z",
     "start_time": "2025-10-17T00:46:55.336129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path = \"D:\\Semantic-Concept-Similarity\\data\\BabelNet\\Visim-400.txt\"\n",
    "df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "words = df[[\"Word1\", \"Word2\"]].values.flatten()\n",
    "output_path = \"D:\\Semantic-Concept-Similarity\\data\\WordNet\\words_list.txt\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for w in words:\n",
    "        f.write(str(w) + \"\\n\")\n",
    "print(\"Đã tạo file:\", output_path)"
   ],
   "id": "57fc43ff421a7efd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã tạo file: D:\\Semantic-Concept-Similarity\\data\\WordNet\\words_list.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\S'\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_4816\\4239384428.py:1: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  file_path = \"D:\\Semantic-Concept-Similarity\\data\\BabelNet\\Visim-400.txt\"\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_4816\\4239384428.py:4: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  output_path = \"D:\\Semantic-Concept-Similarity\\data\\WordNet\\words_list.txt\"\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Truy vấn Word list vào WordNet và lấy gloss của chúng\n",
   "id": "bfe54019495d8b50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T16:00:34.748626Z",
     "start_time": "2025-10-14T16:00:34.601743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "Word_Net = r\"D:\\Semantic-Concept-Similarity\\data\\WordNet\\WordNet.txt\"\n",
    "Word_list = r\"D:\\Semantic-Concept-Similarity\\data\\WordNet\\words_list.txt\"\n",
    "output = r\"D:\\Semantic-Concept-Similarity\\data\\WordNet\\WordNet_Definitions.txt\"\n",
    "\n",
    "with open(Word_list, \"r\", encoding=\"utf-8\") as f:\n",
    "    target_words = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "word_to_gloss = {}\n",
    "\n",
    "with open(Word_Net, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        parts = line.strip().split(\"\\t\", 1)\n",
    "        if len(parts) == 2:\n",
    "            word, gloss = parts\n",
    "        else:\n",
    "            # Nếu dòng không có gloss, gán gloss rỗng\n",
    "            word, gloss = parts[0], \"[]\"\n",
    "        word_to_gloss[word.lower()] = gloss\n",
    "\n",
    "filtered_lines = []\n",
    "for word in target_words:\n",
    "    gloss = word_to_gloss.get(word.lower(), \"[]\")  # Nếu không có gloss thì []\n",
    "    filtered_lines.append(f\"{word}\\t{gloss}\\n\")\n",
    "\n",
    "with open(output, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(filtered_lines)\n",
    "\n",
    "print(f\" Đã trích xuất {len(filtered_lines)} từ (theo đúng thứ tự). File lưu tại: {output}\")\n"
   ],
   "id": "37a176cc95d14e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Đã trích xuất 800 từ (theo đúng thứ tự). File lưu tại: D:\\Semantic-Concept-Similarity\\data\\WordNet\\WordNet_Definitions.txt\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Combine BabelNet WordNet",
   "id": "df6901e1e0b4ef95"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T15:19:10.710045Z",
     "start_time": "2025-10-16T15:19:10.686795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "file1 = r\"D:\\Semantic-Concept-Similarity\\data\\WordNet\\WordNet_Definitions.txt\"\n",
    "file2 = r\"D:\\Semantic-Concept-Similarity\\data\\BabelNet\\BabelNet_Defimitions.txt\"\n",
    "output = r\"D:\\Semantic-Concept-Similarity\\data\\ BabelNet_combine_WordNet\\Word_Babel3.txt\"\n",
    "\n",
    "def read_babel_format(path):\n",
    "    data = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or \",\" not in line:\n",
    "                continue\n",
    "            word, rest = line.split(\",\", 1)\n",
    "            word = word.strip()\n",
    "            glosses = re.findall(r\"'(.*?)'\", rest) # lấy tất cả trong ' '\n",
    "            glosses = [g.strip() for g in glosses if g.strip()]\n",
    "            data[word] = glosses\n",
    "    return data\n",
    "\n",
    "def read_wordnet_format(path):\n",
    "    order = []\n",
    "    data = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if \"\\t\" in line:\n",
    "                word, gloss = line.split(\"\\t\", 1)\n",
    "                word, gloss = word.strip(), gloss.strip()\n",
    "                glosses = [g.strip() for g in re.split(r\";|\\.\", gloss) if g.strip()]\n",
    "                data[word] = glosses\n",
    "                order.append(word)\n",
    "            else:\n",
    "                word = line.strip()\n",
    "                data[word] = []\n",
    "                order.append(word)\n",
    "    return order, data\n",
    "\n",
    "order, data1 = read_wordnet_format(file1)\n",
    "data2 = read_babel_format(file2)\n",
    "\n",
    "combined = {}\n",
    "for word in order:\n",
    "    g1 = data1.get(word, [])\n",
    "    g2 = data2.get(word, [])\n",
    "\n",
    "    if not g1 and not g2:\n",
    "        combined[word] = []\n",
    "    else:\n",
    "        glosses = list(dict.fromkeys(g1 + g2))\n",
    "        combined[word] = glosses\n",
    "\n",
    "# kết quả\n",
    "with open(output, \"w\", encoding=\"utf-8\") as out:\n",
    "    for word in order:\n",
    "        glosses = combined.get(word, [])\n",
    "        if glosses:\n",
    "            gloss_str = \"','\".join(glosses)\n",
    "            out.write(f\"{word},['{gloss_str}']\\n\")\n",
    "        else:\n",
    "            out.write(f\"{word},[]\\n\")\n",
    "\n",
    "print(len(order))\n"
   ],
   "id": "103ac5291c2056b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tiền xử lý dữ liệu\n",
   "id": "6d9fedbdbadd861e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Xét thứ tự 2 dòng 1 , nếu 1 trong 2 không có gloss thì mình sẽ bỏ qua",
   "id": "6982b08c18c75fbe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T15:18:56.291841Z",
     "start_time": "2025-10-16T15:18:56.275467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_file = r\"D:\\Semantic-Concept-Similarity\\data\\ BabelNet_combine_WordNet\\Word_Babel3.txt\"\n",
    "output_file = r\"D:\\Semantic-Concept-Similarity\\data\\ BabelNet_combine_WordNet\\WordNet_BabelNet_Definitions2.txt\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# kết quả lưu vào đây\n",
    "result = []\n",
    "\n",
    "# duyệt 2 dòng 1 lần\n",
    "for i in range(0, len(lines) - 1, 2):\n",
    "    line1 = lines[i]\n",
    "    line2 = lines[i + 1]\n",
    "    if \"['[]']\" in line1 or \"['[]']\" in line2 or \",['[]']\" in line1 or \",['[]']\" in line2:\n",
    "        continue\n",
    "    if \"[]]\" in line1 or \"[]]\" in line2 or line1.endswith(\"[]\") or line2.endswith(\"[]\"):\n",
    "        continue\n",
    "    result.extend([line1, line2])\n",
    "\n",
    "# ghi ra file\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in result:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(f\" Đã lọc xong còn: {len(result)}\")\n"
   ],
   "id": "2e027ec75ed7ad19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Đã lọc xong còn: 602\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tách WordNet_BabelNet_Definitions2.txt ra thành word và các gloss riêng",
   "id": "f68fd1b913a1cf55"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T07:59:51.422732Z",
     "start_time": "2025-10-16T07:59:51.396320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import  re\n",
    "\n",
    "input = r\"D:\\Semantic-Concept-Similarity\\data\\ BabelNet_combine_WordNet\\WordNet_BabelNet_Definitions2.txt\"\n",
    "output1= r\"D:\\Semantic-Concept-Similarity\\data\\ BabelNet_combine_WordNet\\BCW_Word.txt\"\n",
    "output2= r\"D:\\Semantic-Concept-Similarity\\data\\ BabelNet_combine_WordNet\\BCW_Definitions.txt\"\n",
    "\n",
    "with open(input, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "words=[]\n",
    "glosss=[]\n",
    "\n",
    "for line in lines:\n",
    "    try:\n",
    "        #tach tu va gloss\n",
    "        word, gloss = line.split(\",\", 1)\n",
    "        word = word.strip()\n",
    "        gloss = re.sub(r\"[\\[\\]]\",\"\",gloss).strip()\n",
    "        words.append(word)\n",
    "        glosss.append(gloss)\n",
    "    except ValueError:\n",
    "     print(f\"khong tach dc dong:{line}\")\n",
    "\n",
    "with open(output1, \"w\", encoding=\"utf-8\") as fw:\n",
    "    fw.writelines(\"\\n\".join(words))\n",
    "with open(output2, \"w\", encoding=\"utf-8\") as fg:\n",
    "    fg.writelines(\"\\n\".join(glosss))\n",
    "\n",
    "\n",
    "print(len(words))\n"
   ],
   "id": "eb73c3f48eb3e46f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T08:00:19.395068Z",
     "start_time": "2025-10-16T08:00:19.391068Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(glosss))",
   "id": "f1025661afc17646",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f4cac7432b1866e1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
